---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 02-Unsupervised-Learning.md in _episodes_rmd/
title: 'Unsupervised Learning'
author: "Hugo Bowne-Anderson, Jorge Perez de Acha Chavez"
teaching: 45
exercises: 15
questions: 
- "What is principal component analysis (PCA)?"
- "How can I perform PCA in R?"
- "What is clustering?"
objectives: 
- "Know the difference between supervised and unsupervised learning."
- "Learn the advantages of doing dimensionality reduction on a dataset."
- "Know the basics of clustering."
- "Perform the k-means algorithm in R."
- "Learn how to read a confusion matrix."
keypoints: 
- "Supervised and unsupervised learning are different machine learning techniques that are used for different purposes."
- "PCA can help simplify data analysis."
- "Clustering may reveal hidden patterns or groupings in the data."
- "A confusion matrix is a tool that allows us to measure the performance of an algorithm."
output: html_document
---






## Unsupervised Learning I: dimensionality reduction

*Machine learning* is the science and art of giving computers the ability to learn to make decisions from data without being explicitly programmed.

*Unsupervised learning*, in essence, is the machine learning task of uncovering hidden patterns and structures from unlabeled data. For example, a business may wish to group its customers into distinct categories based on their purchasing behavior without knowing in advance what these categories maybe. This is known as clustering, one branch of unsupervised learning.

Aside: *Supervised learning*, which we'll get to soon enough, is the branch of machine learning that involves predicting labels, such as whether a tumour will be *benign* or *malignant*.

Another form of *unsupervised learning*, is _dimensionality reduction_: in the breast cancer dataset, for example, there are too many features to keep track of. What if we could reduce the number of features yet still keep much of the information? 

> ## Discussion
>
> Look at features X3 and X5. Do you think we could reduce them to one feature and keep much of the information?
>
{: .discussion}


Principal component analysis  will extract the features with the largest variance. Here let's take the first two principal components and plot them, coloured by tumour diagnosis.



~~~
# PCA on data
ppv_pca <- preProcess(df, method = c("center", "scale", "pca"))
df_pc <- predict(ppv_pca, df)
# Plot 1st 2 principal components
ggplot(df_pc, aes(x = PC1, y = PC2, colour = X2)) + geom_point()
~~~
{: .language-r}

<img src="../fig/rmd-unnamed-chunk-2-1.png" title="plot of chunk unnamed-chunk-2" alt="plot of chunk unnamed-chunk-2" style="display: block; margin: auto;" />

> ## Note
>
> What PCA essentially does is the following:
> 1. The first step of PCA is to decorrelate your data and this corresponds to a linear transformation of the vector space your data lie in;
> 2. The second step is the actual dimension reduction; what is really happening is that your decorrelation step (the first step above) transforms the features into new and uncorrelated features; this second step then chooses the features that contain most of the information about the data (you'll formalize this soon enough).
>
{: .callout}

You can essentially think about PCA as a form of compression. You can read more about PCA [here](https://www.datacamp.com/community/tutorials/data-science-techniques-dataframed#pca).


## Unsupervised Learning II: clustering

One popular technique in unsupervised learning is _clustering_. Essentially, this is the task of grouping your data points, based on something about them, such as closeness in space. What you're going to do is group the tumour data points into two clusters using an algorithm called k-means, which aims to cluster the data in order to minimize the variances of the clusters.

Cluster your data points using k-means and then we'll compare the results to the actual labels that we know:


~~~
# k-means
km.out <- kmeans(df[,2:10], centers=2, nstart=20)
summary(km.out)
~~~
{: .language-r}



~~~
             Length Class  Mode   
cluster      569    -none- numeric
centers       18    -none- numeric
totss          1    -none- numeric
withinss       2    -none- numeric
tot.withinss   1    -none- numeric
betweenss      1    -none- numeric
size           2    -none- numeric
iter           1    -none- numeric
ifault         1    -none- numeric
~~~
{: .output}



~~~
km.out$cluster
~~~
{: .language-r}



~~~
  [1] 1 1 1 2 1 2 1 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 1 1 1 2 1 2 1 1 2 1 1 2
 [36] 1 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2
 [71] 1 2 1 2 2 2 2 1 1 2 2 2 1 1 2 1 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2
[106] 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 1 1 2 2 2 2 1 2 1 2 2 2 2 1 2 2 2 2 2
[141] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 1 1 2 1 2 2 1 1 2 2 2 2 2 2
[176] 2 2 2 2 2 1 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 1 2 2 1 1 2 2 2 2 1 2 2
[211] 1 2 1 1 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 1 1 2 1 2 2 2 2 1
[246] 2 2 2 2 2 1 2 1 1 1 2 1 2 2 2 1 1 1 2 1 1 2 2 2 2 2 2 1 2 1 2 2 1 2 2
[281] 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2
[316] 2 2 1 2 2 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 1 2 2 2 1 2 2 2 2 2 2
[351] 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2
[386] 2 2 2 2 1 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2
[421] 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 1 2 2 1 2 1 2 2 1 2 1 2 2 2
[456] 2 2 2 2 2 1 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1
[491] 2 1 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 1 2 2 2
[526] 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[561] 2 2 2 1 1 1 1 1 2
~~~
{: .output}

Now that you have a cluster for each tumour (clusters 1 and 2), you can see how well they coincide with the labels that you know. To do this you'll use a cool method called cross-tabulation: a cross-tab is a table that allows you to read off how many data points in clusters 1 and 2 were actually benign or malignant respectively.

Let's do it:



~~~
# Cross-tab of clustering & known labels
CrossTable(df$X2, km.out$cluster)
~~~
{: .language-r}



~~~

 
   Cell Contents
|-------------------------|
|                       N |
| Chi-square contribution |
|           N / Row Total |
|           N / Col Total |
|         N / Table Total |
|-------------------------|

 
Total Observations in Table:  569 

 
             | km.out$cluster 
       df$X2 |         1 |         2 | Row Total | 
-------------|-----------|-----------|-----------|
           B |         2 |       355 |       357 | 
             |    73.851 |    20.579 |           | 
             |     0.006 |     0.994 |     0.627 | 
             |     0.016 |     0.798 |           | 
             |     0.004 |     0.624 |           | 
-------------|-----------|-----------|-----------|
           M |       122 |        90 |       212 | 
             |   124.362 |    34.654 |           | 
             |     0.575 |     0.425 |     0.373 | 
             |     0.984 |     0.202 |           | 
             |     0.214 |     0.158 |           | 
-------------|-----------|-----------|-----------|
Column Total |       124 |       445 |       569 | 
             |     0.218 |     0.782 |           | 
-------------|-----------|-----------|-----------|

 
~~~
{: .output}

> ## Discussion
>
> How well did the k-means do at clustering the tumour data?
>
{: .discussion}

