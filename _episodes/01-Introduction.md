---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 01-Introduction.md in _episodes_rmd/
title: "Introduction"
author: "Jorge Perez de Acha Chavez"
questions: 
- "What is Exploratory Data Analysis (EDA) and why is it useful?"
- "How can I do EDA in R?"
objectives: 
- "Use `caret` to preprocess data."
keypoints: 
- "Plots are always useful tools for getting to know your data."
- "Center and scale your numerical variables using the `caret` package."
output: html_document
---


## Introductory words

This is an Introduction to Machine Learning in R, in which you'll learn the basics of unsupervised learning for _pattern recognition_ and supervised learning for _prediction_. At the end of this workshop, we hope that you'll

* appreciate the importance of performing exploratory data analysis (or EDA) before starting to model your data.
* understand the basics of _unsupervised learning_ and know the examples of principal component analysis (PCA) and k-means clustering.
* understand the basics of _supervised learning_ for prediction and the differences between _classification_ and _regression_.
* understand modern machine learning techniques and principles, such as test train split, k-fold cross validation and regularization.
* be able to write code to implement the above techniques and methodologies using R, caret and glmnet.

## Acknowledgements

This material has come from many conversations, workshops and online courses over the years, most notably the work that I have done at DataCamp. Some of the material is similar to material that I developed for DataCamp's [
Supervised Learning with scikit-learn course](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn), on which I collaborated with Andreas MÃ¼ller and Yashas Roy, along with community articles that I have written, such as [Kaggle Tutorial: EDA & Machine Learning](https://www.datacamp.com/community/tutorials/kaggle-machine-learning-eda) & [Experts' Favorite Data Science Techniques](https://www.datacamp.com/community/tutorials/data-science-techniques-dataframed). Finally, I found time to develop this material due to the 20% community time that I have at DataCamp and am indebted to them for this.



## Getting set up computationally

First you'll install the necessary packages and then load them.


~~~
# Run this cell to install & load the required packages
# If you need to install them, uncomment the lines of code below
#install.packages("tidyverse")
#install.packages("kernlab")
#install.packages("ddalpha")
#install.packages("caret")
#install.packages("GGally")
#install.packages("gmodels")
#install.packages("glmnet", repos = "http://cran.us.r-project.org")
#install.packages("e1071")


# Load packages
library(tidyverse)
library(kernlab)
library(ddalpha)
library(caret)
library(GGally)
library(gmodels)
library(glmnet)
~~~
{: .language-r}



## Loading your data

It's time to import the first dataset that we'll work with, the [Breast Cancer Wisconsin (Diagnostic) Data Set](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29) from the UCI Machine Learning repository.

Do this and check out the first several rows:


~~~
# Load data
df <- read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",
               col_names = FALSE)
# Check out head of dataframe
df %>% head()
~~~
{: .language-r}
> ## Discussion
>
> What are the variables in the dataset? Follow the link to UCI above to find out.
>
{: .discussion}

Before thinking about modeling, have a look at your data. There's no point in throwing a $10^4$ layer convolutional neural network (whatever that means) at your data before you even know what you're dealing with.

You'll first remove the first column, which is the unique identifier of each row:


~~~
# Remove first column 
df <- df[2:32]
# View head
df %>% head()
~~~
{: .language-r}

> ## Question
>
> How many features are there in this dataset?
>
{: .challenge}

> ## Discussion
>
> Why did we want to remove the unique identifier?
>
{: .discussion}

Now there are too many features to plot so you'll plot the first 5 in a pair-plot:
<!-- does this mean we're getting rid of features, or that we'll only plot the first five as opposed to the 32 -->


~~~
# Pair-plot of first 5 features
ggpairs(df[1:5], aes(colour=X2, alpha=0.4))
~~~
{: .language-r}

> ## Discussion
>
> What can you see here?
>
{: .discussion}

Note that the features have widely varying centers and scales (means and standard deviations) so we'll want to center and scale them in some situations. You'll use the caret package for this. You can read more about preprocessing with caret [here](https://topepo.github.io/caret/pre-processing.html#pp).
<!-- it is not entirely clear why it's necessary to perform scaling and centering.
perhaps briefly mention the other situations when center and scaling is not necessary? --> 


~~~
# Center & scale data
ppv <- preProcess(df, method = c("center", "scale"))
df_tr <- predict(ppv, df)
# Summarize first 5 columns
df_tr[1:5] %>% summary()
~~~
{: .language-r}


Now plot the centered & scaled features:


~~~
# Pair-plot of transformed data
ggpairs(df_tr[1:5], aes(colour=X2))
~~~
{: .language-r}

> ## Discussion
>
> How does this compare to your previous pairplot?
>
{: .discussion}
