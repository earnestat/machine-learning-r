---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 05-Supervised-Learning-II.md in _episodes_rmd/
title: 'Supervised Learning II: regression'
author: "Jorge Perez de Acha Chavez"
questions: 
- "What if the target variable is numerical rather than categorical?"
objectives: 
- "Apply regression to predict continuous target variables."
- "Understand the root mean square error (RMSE) and how it relates to regression."
keypoints: 
- "Regression is a useful tool to predict numerical variables."
- "Use RMSE to measure the regression's performance."
output: html_document
---





## Supervised Learning II: regression

In the classification task above, we were attempting to predict a categorical outcome, in this case 'benign' or 'malignant'. Regression, the other type of supervised learning, is one in which you're attempting to predict a continuously varying outcome, such as the price of a house or life expectancy.


~~~
gm <- read_csv("data/gapminder.csv") 
gm %>% head()
~~~
{: .language-r}


Plot life expectancy as a function of fertility:



~~~
ggplot(gm, aes(x=fertility, y=life)) + geom_point()
~~~
{: .language-r}

> ## Discussion
>
> What type of regression model might be useful for modeling the above relationship?
>
{: .discussion}

Now you'll build a linear model for the relationship between life expectancy and fertility. For more on the math of linear models, see [here](https://www.datacamp.com/community/tutorials/data-science-techniques-dataframed#linear).


~~~
mod <- lm(life~fertility, gm)
pred <- predict(mod, gm)
~~~
{: .language-r}

Plot the original data, along with the linear regression:



~~~
{plot(gm$fertility, gm$life)
abline(mod)}
~~~
{: .language-r}

> ## Discussion
>
> Many data scientists and statisticians really dig linear regression over more complex models, often citing the reason that it is interpretable: what could this mean?
>
{: .discussion}

### Compute error

What linear regression does when fitting the line to the data is it minimizes the root mean square error (RMSE). Well, it actually minimizes the mean square error but these amount to the same thing. Compute the RMSE of your linear regression model:



~~~
er <- pred - gm$life
rmse <- sqrt(mean(er^2))
rmse
~~~
{: .language-r}


Now you will build a full linear regression model, using all the variables that are in the dataset:


~~~
mod_full <- lm(life~., gm)
pred_full <- predict(mod_full, gm)
er_full <- pred_full - gm$life
rmse_full <- sqrt(mean(er_full^2))
rmse_full
~~~
{: .language-r}


But recall that this may not signify the RMSE on a new dataset that the model has not seen. For this reason, you'll perform a test train split and compute the RMSE:


~~~
# Set seed for reproducible results
set.seed(42)
# Train test split
inTraining <- createDataPartition(gm$life, p = .75, list=FALSE)
# Create train set
gm_train <- gm[ inTraining,]
# Create test set
gm_test <- gm[-inTraining,]
# Fit model to train set
model <- lm(life ~ ., gm_train)
# Predict on test set
p <- predict(model, gm_test)

#
er <- p - gm_test$life
rmse <- sqrt(mean(er^2))
rmse
~~~
{: .language-r}
